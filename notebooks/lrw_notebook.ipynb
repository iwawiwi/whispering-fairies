{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from turbojpeg import TurboJPEG, TJPF_GRAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = '/home/iwawiwi/research/whispering-fairies/data/lrw_cropped/'\n",
    "label = 'CHIEF'\n",
    "phase = 'train'\n",
    "\n",
    "jpeg = TurboJPEG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = torch.load(os.path.join(ROOT, label, phase, label+'_00001.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(videos.keys())\n",
    "video = videos.get('video')         # encoded JPEG data, 29 frames\n",
    "label = videos.get('label')\n",
    "duration = videos.get('duration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(label))\n",
    "print(label)\n",
    "print(type(duration))\n",
    "print(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(len(video)) + ' frames')\n",
    "frames = []\n",
    "frames = [jpeg.decode(frame, TJPF_GRAY) for frame in video]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(frames[0]))  # w, h, c\n",
    "print(frames[0].shape)\n",
    "img = frames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot img\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.imshow(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack and normalize frames\n",
    "frames = np.stack(frames, axis=0) / 255.0\n",
    "print(len(frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(frames.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import current project directory as module\n",
    "import sys\n",
    "sys.path.append('/home/iwawiwi/research/whispering-fairies/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datamodules.components.lrw_dataset import LRWDataset\n",
    "DATA_ROOT = '/home/iwawiwi/research/whispering-fairies/data/lrw_cropped/'\n",
    "LABEL = '/home/iwawiwi/research/whispering-fairies/data/lrw_labels.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = LRWDataset(DATA_ROOT, LABEL, phase='train')\n",
    "data_test = LRWDataset(DATA_ROOT, LABEL, phase='test')\n",
    "data_val = LRWDataset(DATA_ROOT, LABEL, phase='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data_train))\n",
    "print(len(data_val))\n",
    "print(len(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = data_train[0]['video']\n",
    "print(sample.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 4, kernel_size=3, stride=1, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x).relu() # relu output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random tensor\n",
    "x = torch.randn(1, 1, 4, 4)\n",
    "model = SimpleConvNet()\n",
    "y = model(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test init weight tensor\n",
    "w = torch.empty(3, 5)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.init.kaiming_normal_(w, mode='fan_out', nonlinearity='relu')\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate module\n",
    "for m in model.modules():\n",
    "    # print modulename\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try simple network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple 3d convolutional network\n",
    "class SimpleConvNet(nn.Module):\n",
    "    # init\n",
    "    def __init__(self, in_channel=1, out_channel=64, kernel_size=3, stride=(1, 1, 1), dilation=(1, 2, 2), padding=1):\n",
    "        super().__init__()\n",
    "        # self.conv1 = nn.Conv3d(in_channel, 8, kernel_size=kernel_size, stride=(2, 1, 1), padding=padding, dilation=dilation)\n",
    "        # self.relu = nn.ReLU()\n",
    "        # self.pool1 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(1, 2, 2))\n",
    "        # self.conv2 = nn.Conv3d(8, 16, kernel_size, stride, padding, dilation=dilation)\n",
    "        # self.pool2 = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
    "        # self.conv3 = nn.Conv3d(16, 32, kernel_size, stride, padding, dilation=1)\n",
    "        # self.conv4 = nn.Conv3d(32, out_channel, kernel_size, stride, padding, dilation=1)\n",
    "        # # reduce dimension\n",
    "        # self.conv5 = nn.Conv3d(out_channel, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), padding=0)\n",
    "        # self.conv6 = nn.Conv3d(32, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), padding=0)      # torch.Size([4, 16, 14, 5, 5])\n",
    "        # self.fc1 = nn.Linear(5 * 5 * 14 * 16, 500)\n",
    "        self.conv1 = nn.Conv3d(in_channel, 8, kernel_size=3, stride=(1, 1, 1), padding=(1, 1 ,1), dilation=1)\n",
    "        #self.pool1 = nn.MaxPool3d(kernel_size=(1, 4, 4), stride=(1, 4, 4))\n",
    "        #self.fc1 = nn.Linear(2 * 15 * 11 * 11, 500)\n",
    "\n",
    "    # forward\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # x = self.pool1(self.relu(self.conv1(x)))\n",
    "        # x = self.pool2(self.relu(self.conv2(x)))\n",
    "        # x = self.pool2(self.relu(self.conv3(x)))\n",
    "        # x = self.pool2(self.relu(self.conv4(x)))\n",
    "        # x = self.conv6(self.conv5(x))\n",
    "        # # flatten\n",
    "        # x = x.view(x.size(0), -1)\n",
    "        # x = self.fc1(x).mean(dim=1)\n",
    "        x = self.conv1(x)\n",
    "        #x = self.pool1(x)\n",
    "        #x = x.view(x.size(0), -1)\n",
    "        #x = self.fc1(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def __check_output_shape(self, module, input_shape):\n",
    "        x = torch.rand(input_shape)\n",
    "        # no grad calculation\n",
    "        with torch.no_grad():\n",
    "            y = module(x)\n",
    "        # return shape of y\n",
    "        return y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random tensor\n",
    "vid = torch.rand((4, 29, 1, 88, 88))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dl_train = DataLoader(data_train, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dl_train))\n",
    "vid, tgt = batch['video'], batch['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img.shape)\n",
    "print(tgt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model\n",
    "model = SimpleConvNet()\n",
    "with torch.no_grad():\n",
    "    out = model(vid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out.shape)\n",
    "#print(out.shape[-1] * out.shape[-2] * out.shape[-3] * out.shape[-4] * 500)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9558b2175d3b59bb9e77605643848e3642bc80505cb5aeccf875d64f21049811"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('lightning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
